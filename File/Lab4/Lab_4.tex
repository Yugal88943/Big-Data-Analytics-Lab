% Assignment 4: Develop in-memory analytics capability using Spark.
%-----------------------------------------------
\refstepcounter{section}
\addlabcontentsline{Assignment \thesection: Implement Data Processing using Spark and Compare with MapReduce.}
{(17-02-2026)}
{\thepage}      
\section*{\centering Assignment \thesection: Implement Data Processing using Spark and Compare with MapReduce}

\noindent \textbf{Objective:} Develop in-memory analytics capability by implementing the same data processing task using Apache Spark and comparing performance with MapReduce.

%-----------------------------------------------
% Step 1
%-----------------------------------------------
\subsection{Step 1: Start Hadoop Services}

\begin{verbatim}
su - hadoop
start-dfs.sh
start-yarn.sh
jps
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/start_services.png}
    \caption{Starting Hadoop services and verifying using jps}
    \label{fig:spark_start_services}
\end{figure}

%-----------------------------------------------
% Step 2
%-----------------------------------------------
\subsection{Step 2: Verify Input File in HDFS}

\noindent The same dataset used in MapReduce is reused for Spark processing.

\begin{verbatim}
hdfs dfs -ls /
hdfs dfs -ls /input
hdfs dfs -cat /input/input.txt
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/hdfs_input.png}
    \caption{Verifying input dataset in HDFS}
    \label{fig:spark_input}
\end{figure}

%-----------------------------------------------
% Step 3
%-----------------------------------------------
\subsection{Step 3: Start Spark Shell}

\begin{verbatim}
~/spark-3.5.1-bin-hadoop3/bin/spark-shell
\end{verbatim}

\noindent Spark shell starts successfully and provides the Scala prompt for interactive execution.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/spark_shell.png}
    \caption{Launching Spark Shell}
    \label{fig:spark_shell}
\end{figure}

%-----------------------------------------------
% Step 4
%-----------------------------------------------
\subsection{Step 4: Load Data from HDFS into Spark}

\begin{verbatim}
val lines = sc.textFile("hdfs://localhost:9000/input/input.txt")
lines.collect()
\end{verbatim}

\noindent The dataset stored in HDFS is loaded into an RDD for processing.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/load_data.png}
    \caption{Reading input data from HDFS}
    \label{fig:load_data}
\end{figure}

%-----------------------------------------------
% Step 5
%-----------------------------------------------
\subsection{Step 5: Implement Spark Transformations}

\noindent Word Count is implemented using Spark transformations.

\begin{verbatim}
val words = lines.flatMap(line => line.split(" "))
val wordPairs = words.map(word => (word, 1))
val counts = wordPairs.reduceByKey((a,b) => a+b)
\end{verbatim}

\noindent
\textbf{flatMap()} splits lines into words,  
\textbf{map()} creates key-value pairs, and  
\textbf{reduceByKey()} aggregates counts.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/transformations.png}
    \caption{Spark transformations for Word Count}
    \label{fig:transformations}
\end{figure}

%-----------------------------------------------
% Step 6
%-----------------------------------------------
\subsection{Step 6: Apply Actions and Display Output}

\begin{verbatim}
counts.collect()
\end{verbatim}

\noindent Output obtained:

\begin{verbatim}
(big,4)
(data,3)
(mapreduce,3)
(hadoop,4)
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/collect_output.png}
    \caption{Displaying results using collect()}
    \label{fig:collect_output}
\end{figure}

%-----------------------------------------------
% Step 7
%-----------------------------------------------
\subsection{Step 7: Save Output to HDFS}

\begin{verbatim}
counts.saveAsTextFile("hdfs://localhost:9000/output_spark")
\end{verbatim}

\noindent The processed output is stored in HDFS similar to MapReduce output.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/save_output.png}
    \caption{Saving Spark output to HDFS}
    \label{fig:save_output}
\end{figure}

%-----------------------------------------------
% Step 8
%-----------------------------------------------
\subsection{Step 8: Check Output Directory and Validate Result}

\begin{verbatim}
hdfs dfs -ls /
hdfs dfs -cat /output_spark/part-*
\end{verbatim}

\noindent The output matches the MapReduce results, validating correctness.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/output_validation.png}
    \caption{Validating Spark output in HDFS}
    \label{fig:output_validation}
\end{figure}

%-----------------------------------------------
% Step 9
%-----------------------------------------------
\subsection{Step 9: Stop Spark and Hadoop Services}

\begin{verbatim}
:quit
stop-yarn.sh
stop-dfs.sh
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Lab4/Images/stop_services.png}
    \caption{Stopping Spark session and Hadoop services}
    \label{fig:stop_services}
\end{figure}

%-----------------------------------------------
% Comparison
%-----------------------------------------------
\subsection{Step 10: Comparison with MapReduce}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Feature & MapReduce & Spark \\ \hline
Processing Model & Disk-based & In-memory \\ \hline
Execution Speed & Slower & Faster \\ \hline
Code Complexity & More lines of code & Simple and concise \\ \hline
Iterative Processing & Inefficient & Efficient \\ \hline
Performance & Higher latency & Low latency \\ \hline
\end{tabular}
\caption{Comparison between MapReduce and Spark}
\label{tab:comparison}
\end{table}

%-----------------------------------------------
% Result
%-----------------------------------------------
\subsection{Result}

\noindent The Spark application successfully processed the input dataset using in-memory computation. Word count results were generated and stored in HDFS, matching the output produced by the MapReduce implementation.

%-----------------------------------------------
% Conclusion
%-----------------------------------------------
\subsection{Conclusion}

\noindent This assignment demonstrated in-memory analytics using Apache Spark. Compared to MapReduce, Spark required less code and executed faster due to memory-based processing. The experiment highlights Spark's suitability for high-performance analytics and iterative big data processing tasks.
